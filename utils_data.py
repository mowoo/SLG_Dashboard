import pandas as pd
import os
import re
import datetime
import streamlit as st
from typing import Optional, Tuple

# --- Configuration ---
DATA_FOLDER = "ç›Ÿæˆ°è³‡æ–™åº«"
EXCLUDE_GROUPS = ['å°è™Ÿ', 'æœªåˆ†çµ„']
RADAR_CONFIG = {
    'slave':  {'desc': 'ğŸ‘®â€â™‚ï¸ æŠ“åœ°å¥´', 'merit_op': 'å°æ–¼ <=', 'merit_val': 10000, 'power_op': 'å¤§æ–¼ >=', 'power_val': 25000, 'eff_op': 'å°æ–¼ <=', 'eff_val': 2.0},
    'elite':  {'desc': 'âš”ï¸ æ‰¾æˆ°ç¥', 'merit_op': 'å¤§æ–¼ >=', 'merit_val': 100000, 'power_op': 'å¤§æ–¼ >=', 'power_val': 0, 'eff_op': 'å¤§æ–¼ >=', 'eff_val': 10.0},
    'newbie': {'desc': 'ğŸ‘¶ æ‰¾èŒæ–°', 'merit_op': 'å°æ–¼ <=', 'merit_val': 5000, 'power_op': 'å°æ–¼ <=', 'power_val': 10000, 'eff_op': 'å¤§æ–¼ >=', 'eff_val': 0.0},
    'reset':  {'desc': 'ğŸ”„ é‡ç½®', 'merit_op': 'å¤§æ–¼ >=', 'merit_val': 0, 'power_op': 'å¤§æ–¼ >=', 'power_val': 0, 'eff_op': 'å¤§æ–¼ >=', 'eff_val': 0.0}
}

# --- IO Functions ---
def save_uploaded_file(uploaded_file) -> bool:
    if not os.path.exists(DATA_FOLDER):
        os.makedirs(DATA_FOLDER)
    try:
        file_path = os.path.join(DATA_FOLDER, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        return True
    except Exception as e:
        st.error(f"Error saving file {uploaded_file.name}: {e}")
        return False

@st.cache_data(ttl=300)
def load_data_from_folder() -> pd.DataFrame:
    if not os.path.exists(DATA_FOLDER):
        return pd.DataFrame()
    
    all_data_frames = []
    files = [f for f in os.listdir(DATA_FOLDER) if f.endswith('.csv')]
    
    for filename in files:
        file_path = os.path.join(DATA_FOLDER, filename)
        try:
            df = pd.read_csv(file_path)
            
            # Extract timestamp from filename
            # Format: åŒç›Ÿçµ±è¨ˆYYYYå¹´MMæœˆDDæ—¥HH[æ—¶|æ™‚]mmåˆ†SSç§’.csv
            match = re.search(r'(\d{4})å¹´(\d{2})æœˆ(\d{2})æ—¥(\d{2})[æ—¶|æ™‚](\d{2})åˆ†(\d{2})ç§’', filename)
            if match:
                dt_str = f"{match.group(1)}-{match.group(2)}-{match.group(3)} {match.group(4)}:{match.group(5)}:{match.group(6)}"
                df['ç´€éŒ„æ™‚é–“'] = pd.to_datetime(dt_str)
            else:
                # Fallback or skip if date not found
                # print(f"Warning: Could not extract timestamp from {filename}")
                continue

            all_data_frames.append(df)
        except Exception as e:
            print(f"Error loading {filename}: {e}")
        
    if not all_data_frames:
        return pd.DataFrame()
    
    full_df = pd.concat(all_data_frames, ignore_index=True)
    
    if 'ç´€éŒ„æ™‚é–“' in full_df.columns:
        full_df = full_df.sort_values('ç´€éŒ„æ™‚é–“')
        
    required_cols = ['å‹¢åŠ›å€¼', 'æˆ°åŠŸç¸½é‡', 'åˆ†çµ„']
    missing_cols = [col for col in required_cols if col not in full_df.columns]
    if missing_cols:
        st.error(f"è³‡æ–™ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_cols}ï¼Œè«‹æª¢æŸ¥ä¸Šå‚³çš„ CSV æª”æ¡ˆæ ¼å¼ã€‚")
        return pd.DataFrame()
        
    # Data Cleaning
    full_df['å‹¢åŠ›å€¼'] = full_df['å‹¢åŠ›å€¼'].replace(0, 1) # Avoid division by zero
    full_df['æˆ°åŠŸæ•ˆç‡'] = (full_df['æˆ°åŠŸç¸½é‡'] / full_df['å‹¢åŠ›å€¼']).round(2)
    full_df = full_df[~full_df['åˆ†çµ„'].isin(EXCLUDE_GROUPS)]
    
    return full_df

# --- Calculation Functions ---
@st.cache_data(ttl=300)
def calculate_daily_velocity(df: pd.DataFrame, group_col: Optional[str] = None) -> pd.DataFrame:
    df = df.copy()
    df['date_only'] = df['ç´€éŒ„æ™‚é–“'].dt.date
    
    # Get the last record of each day
    daily_snapshots = df.groupby('date_only')['ç´€éŒ„æ™‚é–“'].max().reset_index()
    df_daily = pd.merge(df, daily_snapshots, on=['date_only', 'ç´€éŒ„æ™‚é–“'], how='inner')
    
    if group_col:
        agged = df_daily.groupby(['ç´€éŒ„æ™‚é–“', group_col])[['æˆ°åŠŸç¸½é‡', 'å‹¢åŠ›å€¼']].sum().reset_index()
        agged = agged.sort_values([group_col, 'ç´€éŒ„æ™‚é–“'])
        agged['time_diff'] = agged.groupby(group_col)['ç´€éŒ„æ™‚é–“'].diff().dt.total_seconds() / 86400
        agged['merit_diff'] = agged.groupby(group_col)['æˆ°åŠŸç¸½é‡'].diff()
        agged['power_diff'] = agged.groupby(group_col)['å‹¢åŠ›å€¼'].diff()
    else:
        agged = df_daily.groupby('ç´€éŒ„æ™‚é–“')[['æˆ°åŠŸç¸½é‡', 'å‹¢åŠ›å€¼']].sum().reset_index()
        agged = agged.sort_values('ç´€éŒ„æ™‚é–“')
        agged['time_diff'] = agged['ç´€éŒ„æ™‚é–“'].diff().dt.total_seconds() / 86400
        agged['merit_diff'] = agged['æˆ°åŠŸç¸½é‡'].diff()
        agged['power_diff'] = agged['å‹¢åŠ›å€¼'].diff()
        
    agged['daily_merit_growth'] = (agged['merit_diff'] / agged['time_diff']).fillna(0)
    agged['daily_power_growth'] = (agged['power_diff'] / agged['time_diff']).fillna(0)
    
    return agged

def get_individual_global_max(raw_df: pd.DataFrame) -> Tuple[float, float, float]:
    temp_df = calculate_daily_velocity(raw_df, group_col='æˆå“¡')
    g_max_m = temp_df['daily_merit_growth'].max()
    g_max_p = temp_df['daily_power_growth'].max()
    g_min_p = temp_df['daily_power_growth'].min()
    return g_max_m, g_max_p, g_min_p