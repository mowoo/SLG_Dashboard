import pandas as pd
import os
import re
import datetime
import streamlit as st
from typing import Optional, Tuple

# --- Configuration ---
DATA_FOLDER = "ç›Ÿæˆ°è³‡æ–™åº«"
EXCLUDE_GROUPS = ['å°è™Ÿ', 'æœªåˆ†çµ„']
RADAR_CONFIG = {
    'slave':  {'desc': 'ğŸ‘®â€â™‚ï¸ æŠ“åœ°å¥´', 'merit_op': 'å°æ–¼ <=', 'merit_val': 10000, 'power_op': 'å¤§æ–¼ >=', 'power_val': 25000, 'eff_op': 'å°æ–¼ <=', 'eff_val': 2.0},
    'elite':  {'desc': 'âš”ï¸ æ‰¾æˆ°ç¥', 'merit_op': 'å¤§æ–¼ >=', 'merit_val': 100000, 'power_op': 'å¤§æ–¼ >=', 'power_val': 0, 'eff_op': 'å¤§æ–¼ >=', 'eff_val': 10.0},
    'newbie': {'desc': 'ğŸ‘¶ æ‰¾èŒæ–°', 'merit_op': 'å°æ–¼ <=', 'merit_val': 5000, 'power_op': 'å°æ–¼ <=', 'power_val': 10000, 'eff_op': 'å¤§æ–¼ >=', 'eff_val': 0.0},
    'reset':  {'desc': 'ğŸ”„ é‡ç½®', 'merit_op': 'å¤§æ–¼ >=', 'merit_val': 0, 'power_op': 'å¤§æ–¼ >=', 'power_val': 0, 'eff_op': 'å¤§æ–¼ >=', 'eff_val': 0.0}
}

# --- IO Functions ---
def save_uploaded_file(uploaded_file) -> bool:
    if not os.path.exists(DATA_FOLDER):
        os.makedirs(DATA_FOLDER)
    try:
        file_path = os.path.join(DATA_FOLDER, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        return True
    except Exception as e:
        st.error(f"Error saving file {uploaded_file.name}: {e}")
        return False

@st.cache_data(ttl=300)
def load_data_from_folder() -> pd.DataFrame:
    if not os.path.exists(DATA_FOLDER):
        return pd.DataFrame()
    
    all_data_frames = []
    files = [f for f in os.listdir(DATA_FOLDER) if f.endswith('.csv')]
    
    for filename in files:
        file_path = os.path.join(DATA_FOLDER, filename)
        df = pd.DataFrame()
        
        # 1. å˜—è©¦å¤šç¨®ç·¨ç¢¼è®€å–
        encodings_to_try = ['utf-8-sig', 'utf-8', 'big5', 'gbk']
        for enc in encodings_to_try:
            try:
                df = pd.read_csv(file_path, encoding=enc)
                break
            except:
                continue
        
        if df.empty:
            st.warning(f"âš ï¸ ç„¡æ³•è®€å–æª”æ¡ˆ {filename} (ç·¨ç¢¼å¤±æ•—)")
            continue

        # 2. æ¸…æ´—æ¬„ä½åç¨± (å»é™¤ç©ºç™½)
        df.columns = df.columns.str.strip()

        # 3. æ¬„ä½åˆ¥åè‡ªå‹•å°æ‡‰ (Mapping)
        # å¦‚æœ CSV æ˜¯ç°¡é«”æˆ–åˆ¥åï¼Œè‡ªå‹•è½‰å›æ¨™æº–åç¨±
        column_mapping = {
            'åŠ¿åŠ›å€¼': 'å‹¢åŠ›å€¼', 'åŠ¿åŠ›': 'å‹¢åŠ›å€¼', 'Power': 'å‹¢åŠ›å€¼',
            'æˆ˜åŠŸæ€»é‡': 'æˆ°åŠŸç¸½é‡', 'æˆ˜åŠŸ': 'æˆ°åŠŸç¸½é‡', 'Merit': 'æˆ°åŠŸç¸½é‡',
            'åˆ†ç»„': 'åˆ†çµ„', 'Group': 'åˆ†çµ„',
            'æˆå‘˜': 'æˆå“¡', 'Member': 'æˆå“¡',
            'æ‰€å±åŠ¿åŠ›': 'æ‰€å±¬å‹¢åŠ›', 'Region': 'æ‰€å±¬å‹¢åŠ›'
        }
        df.rename(columns=column_mapping, inplace=True)

        # 4. è®€å–æ™‚é–“æˆ³
        match = re.search(r'(\d{4})å¹´(\d{2})æœˆ(\d{2})æ—¥(\d{2})[æ—¶|æ™‚](\d{2})åˆ†(\d{2})ç§’', filename)
        if match:
            dt_str = f"{match.group(1)}-{match.group(2)}-{match.group(3)} {match.group(4)}:{match.group(5)}:{match.group(6)}"
            df['ç´€éŒ„æ™‚é–“'] = pd.to_datetime(dt_str)
        else:
            # æª”åæ²’æ™‚é–“ï¼Œè·³é
            continue

        all_data_frames.append(df)
        
    if not all_data_frames:
        return pd.DataFrame()
    
    full_df = pd.concat(all_data_frames, ignore_index=True)
    if 'ç´€éŒ„æ™‚é–“' in full_df.columns:
        full_df = full_df.sort_values('ç´€éŒ„æ™‚é–“')
        
    # 5. æœ€çµ‚æª¢æŸ¥èˆ‡é™¤éŒ¯è¼¸å‡º
    required_cols = ['å‹¢åŠ›å€¼', 'æˆ°åŠŸç¸½é‡', 'åˆ†çµ„']
    missing_cols = [col for col in required_cols if col not in full_df.columns]
    
    if missing_cols:
        st.error("âŒ CSV æ¬„ä½è®€å–å¤±æ•—")
        st.markdown(f"**ç¼ºå°‘çš„æ¬„ä½:** `{missing_cols}`")
        st.markdown(f"**å¯¦éš›è®€åˆ°çš„æ¬„ä½ (å‰5å€‹):** `{list(full_df.columns)[:5]} ...`")
        st.warning("è«‹æˆªåœ–æ­¤ç•«é¢ï¼Œæˆ–æª¢æŸ¥ CSV æ¨™é¡Œæ˜¯å¦ç‚ºäº‚ç¢¼ (Big5/GBK)ã€‚")
        return pd.DataFrame()
        
    # Data Cleaning
    for col in ['å‹¢åŠ›å€¼', 'æˆ°åŠŸç¸½é‡']:
        full_df[col] = pd.to_numeric(full_df[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0)

    full_df['å‹¢åŠ›å€¼'] = full_df['å‹¢åŠ›å€¼'].replace(0, 1)
    full_df['æˆ°åŠŸæ•ˆç‡'] = (full_df['æˆ°åŠŸç¸½é‡'] / full_df['å‹¢åŠ›å€¼']).round(2)
    full_df = full_df[~full_df['åˆ†çµ„'].isin(EXCLUDE_GROUPS)]
    
    return full_df

# --- Calculation Functions ---
@st.cache_data(ttl=300)
def calculate_daily_velocity(df: pd.DataFrame, group_col: Optional[str] = None) -> pd.DataFrame:
    df = df.copy()
    df['date_only'] = df['ç´€éŒ„æ™‚é–“'].dt.date
    
    daily_snapshots = df.groupby('date_only')['ç´€éŒ„æ™‚é–“'].max().reset_index()
    df_daily = pd.merge(df, daily_snapshots, on=['date_only', 'ç´€éŒ„æ™‚é–“'], how='inner')
    
    if group_col:
        agged = df_daily.groupby(['ç´€éŒ„æ™‚é–“', group_col])[['æˆ°åŠŸç¸½é‡', 'å‹¢åŠ›å€¼']].sum().reset_index()
        agged = agged.sort_values([group_col, 'ç´€éŒ„æ™‚é–“'])
        agged['time_diff'] = agged.groupby(group_col)['ç´€éŒ„æ™‚é–“'].diff().dt.total_seconds() / 86400
        agged['merit_diff'] = agged.groupby(group_col)['æˆ°åŠŸç¸½é‡'].diff()
        agged['power_diff'] = agged.groupby(group_col)['å‹¢åŠ›å€¼'].diff()
    else:
        agged = df_daily.groupby('ç´€éŒ„æ™‚é–“')[['æˆ°åŠŸç¸½é‡', 'å‹¢åŠ›å€¼']].sum().reset_index()
        agged = agged.sort_values('ç´€éŒ„æ™‚é–“')
        agged['time_diff'] = agged['ç´€éŒ„æ™‚é–“'].diff().dt.total_seconds() / 86400
        agged['merit_diff'] = agged['æˆ°åŠŸç¸½é‡'].diff()
        agged['power_diff'] = agged['å‹¢åŠ›å€¼'].diff()
        
    agged['daily_merit_growth'] = (agged['merit_diff'] / agged['time_diff']).fillna(0)
    agged['daily_power_growth'] = (agged['power_diff'] / agged['time_diff']).fillna(0)
    
    return agged

def get_individual_global_max(raw_df: pd.DataFrame) -> Tuple[float, float, float]:
    temp_df = calculate_daily_velocity(raw_df, group_col='æˆå“¡')
    g_max_m = temp_df['daily_merit_growth'].max()
    g_max_p = temp_df['daily_power_growth'].max()
    g_min_p = temp_df['daily_power_growth'].min()
    return g_max_m, g_max_p, g_min_p